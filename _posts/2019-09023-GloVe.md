---
layout: post
title: "GloVe: Global Vectors for Word Representation"
date: 2019-09-23
---

The paper introduces a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. The model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix.
The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.

**The GloVe Model**

Using the insights a new model is constructed for word representation which is called GloVe, for Global Vectors, because the global corpus statistics are captured directly by the model. 
First we establish some notation. Let the matrix of word-word co-occurrence  counts be denoted by _X_, whose entries _Xᵢⱼ_ tabulate the number of times word _j_ occurs in the context of word _i_.  Let _Xᵢ_ = ∑<sub>k</sub>Xᵢ<sub>k</sub> be the number of times any word appears in the context of word _i_. Finally, let Pᵢⱼ = P(_j_|_i_) = _Xᵢⱼ_ / _Xᵢ_ be the probability that word _j_ appear in the context of word _i_. 
For example consider two words _i_ and _j_ that exhibit a particular aspect of interest; for concreteness, suppose we are interested in the concept of thermodynamics phase, for which we might take _i_ = _ice_ and _j_ = _steam_. The relationship of these words can be examined by studying the ration of their co-occurrence probabilities with various probe words, _k_. For words related to ice but not steam, say _k_ = _solid_, we expect the ratio _Pᵢ<sub>k</sub> / P<sub>jk</sub>_ will be large. Similarly, for words _k_ related to ice but not steam, say _k_ = _gas_, the ratio should be small. For words _k_ like _water_ or _fashion_, that are either related to both ice and steam, or to neither, the ratio should be close to one.
The appropriate starting point for word vector learning should be with ratio of co-occurrence probabilities. Noting that the ratio P<sub>ik</sub>_P<sub>jk</sub> depends on three words /i_ , _j_  and _k_, the most general model takes the form,
![an image alt text]({{ site.baseurl }}/images/1.png "an image title")
where w ∈ R d  and word vectors and w ∈ R d are separate context word vectors. We can restrict out consideration to those functions F that depend only on the difference of the two target words, modifying Eqn.(1) to,
![an image alt text]({{ site.baseurl }}/images/2.png "an image title")
The argument of F in Eqn.(2) are vectors while the right-hand side is a scalar.
We can take the dot product of the arguments which prevents F from mixing the vector dimension in undesirable ways.
![an image alt text]({{ site.baseurl }}/images/3.png "an image title")
Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles. To do so consistently, we must not only exchange w ↔ w˜ but also X ↔ X T . Our final model should be invariant under this relabeling, but Eqn. (3) is not. However, the symmetry can be restored in two steps. First, we require that F be a homomorphism between the groups (R,+) and (R>0, ×), i.e.,
![an image alt text]({{ site.baseurl }}/images/4.png "an image title")
which, by Eqn. (3), is solved by,
![an image alt text]({{ site.baseurl }}/images/5.png "an image title")
The solution to Eqn. (4) is F = exp, or,
![an image alt text]({{ site.baseurl }}/images/6.png "an image title")
Next, we note that Eqn. (6) would exhibit the exchange symmetry if not for the log(X<sub>i</sub>) on the right-hand side. However, this term is independent of k so it can be absorbed into a bias b<sub>i</sub> for w<sub>i</sub> . Finally, adding an additional bias b˜ <sub>k</sub> for ˜w<sub>k</sub> restores the symmetry,
![an image alt text]({{ site.baseurl }}/images/7.png "an image title")
Eqn. (7) is a drastic simplification over Eqn. (1), but it is actually ill-defined since the logarithm diverges whenever its argument is zero. One resolution to this issue is to include an additive shift in the logarithm, log(X<sub>ik</sub>) → log(1 + X<sub>ik</sub> ), which maintains the sparsity of X while avoiding the divergences. A main drawback to this model is that it weighs all co-occurrences equally, even those that happen rarely or never.
We propose a new weighted least squares regression model that addresses these problems. Casting Eqn. (7) as a least squares problem and introducing a weighting function f (X<sub>ij</sub>) into the cost function gives us the model
![an image alt text]({{ site.baseurl }}/images/8.png "an image title")
where V is the size of the vocabulary. The weighting function should obey the following properties: 
1. f (0) = 0. If f is viewed as a continuous function, it should vanish as x → 0 fast enough that the limx→0 f (x) log2 x is finite. 
2. f (x) should be non-decreasing so that rare co-occurrences are not overweighted. 
3. f (x) should be relatively small for large values of x, so that frequent co-occurrences are not overweighted. 

The performance of the model depends weakly on the cutoff, which we fix to xmax = 100 for all our experiments. We found that α = 3/4 gives a modest improvement over a linear version with α = 1.
As can be seen from Eqn. (8) and the explicit form of the weighting function f (X), the computational complexity of the model depends on the number of nonzero elements in the matrix X. As this number is always less than the total number of entries of the matrix, the model scales no worse than O(|V | ²).
 
 